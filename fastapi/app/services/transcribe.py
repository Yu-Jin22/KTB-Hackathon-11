"""
ìŒì„± ì¸ì‹(STT) ì„œë¹„ìŠ¤ ëª¨ë“ˆ.

OpenAI Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: gpt-4o-transcribe (ì •í™•ë„) + whisper-1 (íƒ€ì„ìŠ¤íƒ¬í”„) ë³‘í•©
"""
import asyncio
import logging
import re
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import Any, Dict, List

import httpx
from openai import OpenAI

from app.config import (
    MAX_AUDIO_FILE_SIZE,
    OPENAI_API_KEY,
    SUPPORTED_AUDIO_FORMATS,
)
from app.exceptions import AudioFileError, TranscriptionError
from app.prompts import COOKING_PROMPT

# =============================================================================
# ë¡œê¹… ë° í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# =============================================================================
logger = logging.getLogger(__name__)

# íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì˜¤ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ + ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤)
# connect: ì—°ê²° íƒ€ì„ì•„ì›ƒ, read: ì‘ë‹µ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ
http_client = httpx.Client(timeout=httpx.Timeout(connect=30.0, read=300.0, write=30.0, pool=30.0))
client = OpenAI(api_key=OPENAI_API_KEY, http_client=http_client)

# =============================================================================
# ìƒìˆ˜
# =============================================================================
MODEL_ACCURATE = "gpt-4o-transcribe"  # ì •í™•í•œ í…ìŠ¤íŠ¸ìš©
MODEL_TIMESTAMP = "whisper-1"          # íƒ€ì„ìŠ¤íƒ¬í”„ìš©


# =============================================================================
# í…ìŠ¤íŠ¸ ì²˜ë¦¬ íŒ¨í„´ (pre-compiled)
# =============================================================================
# ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ (í•œêµ­ì–´)
SENTENCE_ENDINGS = re.compile(
    r"(ìš”|ë‹¤|ì£ |ë„¤ìš”|ì„¸ìš”|í•´ìš”|í•˜ì„¸ìš”|í•©ë‹ˆë‹¤|ë©ë‹ˆë‹¤|ì…ë‹ˆë‹¤|ìˆì–´ìš”|ì—†ì–´ìš”|"
    r"ì£¼ì„¸ìš”|ë“œì„¸ìš”|ë„£ìœ¼ì„¸ìš”|ë³¶ìœ¼ì„¸ìš”|ì°ì–´ì£¼ì„¸ìš”|êµ¬ì›Œì£¼ì„¸ìš”|ë“ì—¬ì£¼ì„¸ìš”|"
    r"ê±°ë“ ìš”|ì–ì•„ìš”|ëŒ€ìš”|ë˜ìš”|ëƒê³ ìš”|ëŠ”ë°ìš”|ì–´ìš”|ì•„ìš”|"
    r"ê³ ìš”|êµ¬ìš”|ë‚˜ìš”|ê¹Œìš”|ã„¹ê¹Œìš”|ì„ê¹Œìš”|"
    r"ë‹ˆë‹¤|ã…‚ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|"
    r"ê±°ì˜ˆìš”|ê±´ë°ìš”|ì„¸ìš”|ë„¤ìš”|ì£ |ì–´|ì•¼)[\.\!\?]?$"
)


# =============================================================================
# íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
# =============================================================================
def _validate_audio_file(audio_path: str) -> None:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬.

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

    Raises:
        AudioFileError: íŒŒì¼ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
    """
    path = Path(audio_path)

    if not path.exists():
        raise AudioFileError(
            f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}"
        )

    file_size = path.stat().st_size
    if file_size == 0:
        raise AudioFileError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {audio_path}")

    if file_size > MAX_AUDIO_FILE_SIZE:
        raise AudioFileError(
            f"ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤: {file_size / 1024 / 1024:.1f}MB "
            f"(ìµœëŒ€ {MAX_AUDIO_FILE_SIZE / 1024 / 1024}MB)"
        )

    if path.suffix.lower() not in SUPPORTED_AUDIO_FORMATS:
        raise AudioFileError(
            f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜¤ë””ì˜¤ í˜•ì‹ì…ë‹ˆë‹¤: {path.suffix} "
            f"(ì§€ì› í˜•ì‹: {', '.join(SUPPORTED_AUDIO_FORMATS)})"
        )


# =============================================================================
# í…ìŠ¤íŠ¸ ì •ì œ
# =============================================================================
def _clean_transcript_text(text: str) -> str:
    """
    ì „ì‚¬ í…ìŠ¤íŠ¸ ì •ì œ (ë°˜ë³µ, í•„ëŸ¬ ë‹¨ì–´, ì˜¤ì¸ì‹ íŒ¨í„´ ì œê±°).

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸

    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""

    # í•„ëŸ¬ ë‹¨ì–´ ì œê±°
    text = re.sub(r"\b(ìŒ+|ì–´+|ê·¸+|ì•„+|ì—+)\.{0,3}\s*", "", text)

    # ë°˜ë³µë˜ëŠ” ê°íƒ„ì‚¬ ì œê±°
    text = re.sub(r"\b(ë„¤ë„¤|ì•„ì•„|ì˜¤ì˜¤|ì™€ì™€|ìŒìŒ|ì–´ì–´)\b", "", text)

    # ì˜ë¯¸ì—†ëŠ” ë°˜ë³µ íŒ¨í„´ ì œê±°
    text = re.sub(r"\b(\w+)(\s+\1){2,}\b", r"\1", text)

    # Whisper ì˜¤ì¸ì‹ íŒ¨í„´ êµì • (ìœ íŠœë¸Œ ê´€ë ¨)
    text = re.sub(r"êµ¬ë…\s*ì¢‹ì•„ìš”\s*ì•Œë¦¼", "", text)
    text = re.sub(r"êµ¬ë…ê³¼\s*ì¢‹ì•„ìš”", "", text)

    # ë°°ê²½ìŒì•… ì¸ì‹ ì˜¤ë¥˜ ì œê±°
    text = re.sub(r"â™ª+|â™«+|ğŸµ+|ğŸ¶+", "", text)
    text = re.sub(
        r"\[ìŒì•…\]|\[ë°°ê²½ìŒì•…\]|\[BGM\]",
        "",
        text,
        flags=re.IGNORECASE
    )

    # ìˆ«ì+ë‹¨ìœ„ ì •ê·œí™”
    text = re.sub(r"(\d+)\s*ìŠ¤í‘¼", r"\1ìŠ¤í‘¼", text)
    text = re.sub(r"(\d+)\s*í°ìˆ ", r"\1í°ìˆ ", text)
    text = re.sub(r"(\d+)\s*ì‘ì€ìˆ ", r"\1ì‘ì€ìˆ ", text)
    text = re.sub(r"(\d+)\s*ë¶„", r"\1ë¶„", text)
    text = re.sub(r"(\d+)\s*ì´ˆ", r"\1ì´ˆ", text)
    text = re.sub(r"(\d+)\s*ê·¸ë¨", r"\1g", text)
    text = re.sub(r"(\d+)\s*g", r"\1g", text)
    text = re.sub(r"(\d+)\s*ml", r"\1ml", text, flags=re.IGNORECASE)

    # ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r"\s+", " ", text)

    # ë¬¸ì¥ ë¶€í˜¸ ì •ë¦¬
    text = re.sub(r"\.{2,}", ".", text)
    text = re.sub(r"\s+([,.!?])", r"\1", text)

    return text.strip()


# =============================================================================
# ë¬¸ì¥ ë¶„ë¦¬
# =============================================================================
def _split_into_sentences_from_segments(
    segments: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Whisper API ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.

    Args:
        segments: Whisper API ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì •ë¦¬ëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if not segments:
        return []

    result = []
    for seg in segments:
        text = seg.get("text", "").strip()
        if text:
            result.append({
                "start": round(seg.get("start", 0), 2),
                "end": round(seg.get("end", 0), 2),
                "text": _clean_transcript_text(text)
            })

    return result


def _split_text_into_sentences(
    full_text: str,
    duration: float
) -> List[Dict[str, Any]]:
    """
    ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ëŠ” ê²½ìš°).

    Args:
        full_text: ì „ì²´ í…ìŠ¤íŠ¸
        duration: ì˜¤ë””ì˜¤ ê¸¸ì´

    Returns:
        ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if not full_text:
        return []

    # ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
    sentences = re.split(r'(?<=[.!?ìš”ë‹¤ì£ ])\s+', full_text)
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return [{"start": 0, "end": duration, "text": full_text}]

    # ê· ë“±í•˜ê²Œ íƒ€ì„ìŠ¤íƒ¬í”„ ë°°ë¶„
    time_per_sentence = duration / len(sentences) if sentences else 0
    result = []

    for i, sentence in enumerate(sentences):
        result.append({
            "start": round(i * time_per_sentence, 2),
            "end": round((i + 1) * time_per_sentence, 2),
            "text": _clean_transcript_text(sentence)
        })

    return result


# =============================================================================
# OpenAI Whisper API í˜¸ì¶œ
# =============================================================================
def _transcribe_accurate(audio_path: str, language: str = "ko") -> Dict[str, Any]:
    """
    gpt-4o-transcribe ëª¨ë¸ë¡œ ì •í™•í•œ í…ìŠ¤íŠ¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        language: ì–¸ì–´ ì½”ë“œ

    Returns:
        ì „ì‚¬ ê²°ê³¼ (textë§Œ í¬í•¨)
    """
    print("[STT] gpt-4o-transcribe API í˜¸ì¶œ ì¤‘...")
    logger.info(f"[STT] gpt-4o-transcribe API í˜¸ì¶œ: {audio_path}")

    with open(audio_path, "rb") as audio_file:
        response = client.audio.transcriptions.create(
            model=MODEL_ACCURATE,
            file=audio_file,
            language=language,
            response_format="json",
            prompt=COOKING_PROMPT
        )

    return response


def _transcribe_with_timestamps(
    audio_path: str,
    language: str = "ko"
) -> Dict[str, Any]:
    """
    whisper-1 ëª¨ë¸ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ëœ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì–»ìŠµë‹ˆë‹¤.

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        language: ì–¸ì–´ ì½”ë“œ

    Returns:
        ì „ì‚¬ ê²°ê³¼ (segments, duration í¬í•¨)
    """
    print("[STT] whisper-1 API í˜¸ì¶œ ì¤‘ (íƒ€ì„ìŠ¤íƒ¬í”„)...")
    logger.info(f"[STT] whisper-1 API í˜¸ì¶œ (íƒ€ì„ìŠ¤íƒ¬í”„ìš©): {audio_path}")

    with open(audio_path, "rb") as audio_file:
        response = client.audio.transcriptions.create(
            model=MODEL_TIMESTAMP,
            file=audio_file,
            language=language,
            response_format="verbose_json",
            prompt=COOKING_PROMPT
        )

    return response


def _merge_transcripts(
    accurate_text: str,
    timestamp_response: Any
) -> Dict[str, Any]:
    """
    gpt-4o-transcribe í…ìŠ¤íŠ¸ì™€ whisper-1 íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.

    ì „ëµ:
    - full_text: gpt-4o-transcribeì˜ ì •í™•í•œ í…ìŠ¤íŠ¸ ì‚¬ìš©
    - segments: whisper-1ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë˜,
                ê° ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ë¥¼ gpt-4o-transcribe í…ìŠ¤íŠ¸ì—ì„œ ë§¤ì¹­

    Args:
        accurate_text: gpt-4o-transcribeì—ì„œ ì–»ì€ ì •í™•í•œ í…ìŠ¤íŠ¸
        timestamp_response: whisper-1ì—ì„œ ì–»ì€ íƒ€ì„ìŠ¤íƒ¬í”„ ì‘ë‹µ

    Returns:
        ë³‘í•©ëœ ì „ì‚¬ ê²°ê³¼
    """
    # whisper-1 ì‘ë‹µì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ì™€ duration ì¶”ì¶œ
    segments = []
    if hasattr(timestamp_response, 'segments') and timestamp_response.segments:
        segments = timestamp_response.segments

    duration = timestamp_response.duration if hasattr(
        timestamp_response, 'duration'
    ) else 0

    if not segments:
        # ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ
        return {
            "segments": [{"start": 0, "end": duration, "text": accurate_text}],
            "duration": duration
        }

    # ì •í™•í•œ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    accurate_sentences = re.split(r'(?<=[.!?ìš”ë‹¤ì£ ])\s*', accurate_text)
    accurate_sentences = [s.strip() for s in accurate_sentences if s.strip()]

    # whisper-1 ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ë„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ë§¤í•‘ ì¤€ë¹„
    whisper_sentences = []
    for seg in segments:
        text = seg.text if hasattr(seg, 'text') else seg.get('text', '')
        whisper_sentences.append({
            "start": seg.start if hasattr(seg, 'start') else seg.get('start', 0),
            "end": seg.end if hasattr(seg, 'end') else seg.get('end', 0),
            "text": text.strip()
        })

    # ë³‘í•© ì „ëµ: whisper-1ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ êµ¬ì¡° ìœ ì§€, í…ìŠ¤íŠ¸ëŠ” ë¹„ìœ¨ì— ë§ê²Œ ë°°ë¶„
    merged_segments = []

    if len(accurate_sentences) == len(whisper_sentences):
        # ë¬¸ì¥ ìˆ˜ê°€ ê°™ìœ¼ë©´ 1:1 ë§¤í•‘
        for i, ws in enumerate(whisper_sentences):
            merged_segments.append({
                "start": round(ws["start"], 2),
                "end": round(ws["end"], 2),
                "text": accurate_sentences[i]
            })
    else:
        # ë¬¸ì¥ ìˆ˜ê°€ ë‹¤ë¥´ë©´ whisper-1 íƒ€ì„ìŠ¤íƒ¬í”„ì— gpt-4o í…ìŠ¤íŠ¸ë¥¼ ë¹„ìœ¨ ë°°ë¶„
        total_whisper_len = sum(len(ws["text"]) for ws in whisper_sentences)

        if total_whisper_len == 0:
            # whisper í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê· ë“± ë°°ë¶„
            time_per_sentence = duration / len(accurate_sentences)
            for i, sent in enumerate(accurate_sentences):
                merged_segments.append({
                    "start": round(i * time_per_sentence, 2),
                    "end": round((i + 1) * time_per_sentence, 2),
                    "text": sent
                })
        else:
            # whisper ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ë¹„ìœ¨ì— ë”°ë¼ accurate í…ìŠ¤íŠ¸ ë°°ë¶„
            accurate_full = " ".join(accurate_sentences)
            accurate_idx = 0

            for ws in whisper_sentences:
                # ì´ ì„¸ê·¸ë¨¼íŠ¸ì— í• ë‹¹í•  ë¬¸ì ìˆ˜ ê³„ì‚°
                ratio = len(ws["text"]) / total_whisper_len
                chars_to_assign = int(len(accurate_full) * ratio)

                # ë¬¸ì¥ ê²½ê³„ì—ì„œ ëŠê¸°
                end_idx = min(accurate_idx + chars_to_assign, len(accurate_full))

                # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ë¬¸ì¥ ì¢…ê²° ìœ„ì¹˜ ì°¾ê¸°
                if end_idx < len(accurate_full):
                    # ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì¥ ì¢…ê²° ì°¾ê¸°
                    for delim in ['. ', '! ', '? ', 'ìš” ', 'ë‹¤ ', 'ì£  ']:
                        pos = accurate_full.find(delim, end_idx - 20, end_idx + 20)
                        if pos != -1:
                            end_idx = pos + len(delim)
                            break

                segment_text = accurate_full[accurate_idx:end_idx].strip()
                accurate_idx = end_idx

                if segment_text:
                    merged_segments.append({
                        "start": round(ws["start"], 2),
                        "end": round(ws["end"], 2),
                        "text": segment_text
                    })

            # ë‚¨ì€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ì— ì¶”ê°€
            if accurate_idx < len(accurate_full) and merged_segments:
                remaining = accurate_full[accurate_idx:].strip()
                if remaining:
                    merged_segments[-1]["text"] += " " + remaining

    return {
        "segments": merged_segments,
        "duration": duration
    }


# =============================================================================
# ë©”ì¸ ì „ì‚¬ í•¨ìˆ˜
# =============================================================================
async def transcribe_audio(
    audio_path: str,
    language: str = "ko",
    use_vad: bool = False  # API ëª¨ë“œì—ì„œëŠ” VAD ë¯¸ì‚¬ìš©
) -> Dict[str, Any]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    ë‘ ëª¨ë¸ì„ ë³‘ë ¬ë¡œ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ë³‘í•©:
    - gpt-4o-transcribe: ì •í™•í•œ í…ìŠ¤íŠ¸
    - whisper-1: íƒ€ì„ìŠ¤íƒ¬í”„/ì„¸ê·¸ë¨¼íŠ¸

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        language: ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸: ko)
        use_vad: VAD ì‚¬ìš© ì—¬ë¶€ (API ëª¨ë“œì—ì„œëŠ” ë¬´ì‹œë¨)

    Returns:
        ì „ì‚¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬:
        - full_text: ì „ì²´ í…ìŠ¤íŠ¸ (gpt-4o-transcribe)
        - segments: ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ (íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” whisper-1, í…ìŠ¤íŠ¸ëŠ” ë³‘í•©)
        - language: ì–¸ì–´ ì½”ë“œ
        - duration: ì˜¤ë””ì˜¤ ê¸¸ì´

    Raises:
        TranscriptionError: ìŒì„± ì¸ì‹ ì‹¤íŒ¨ ì‹œ
        AudioFileError: ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
    """
    _validate_audio_file(audio_path)

    print("[STT] í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ ì‹œì‘...")
    logger.info(f"[STT] í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ ì‹œì‘: {audio_path}")

    try:
        # ë‘ APIë¥¼ ë³‘ë ¬ë¡œ í˜¸ì¶œ
        loop = asyncio.get_event_loop()

        with ThreadPoolExecutor(max_workers=2) as executor:
            # gpt-4o-transcribe (ì •í™•í•œ í…ìŠ¤íŠ¸)
            accurate_future = loop.run_in_executor(
                executor,
                _transcribe_accurate,
                audio_path,
                language
            )
            # whisper-1 (íƒ€ì„ìŠ¤íƒ¬í”„)
            timestamp_future = loop.run_in_executor(
                executor,
                _transcribe_with_timestamps,
                audio_path,
                language
            )

            # ë‘ ê²°ê³¼ ëŒ€ê¸°
            accurate_response, timestamp_response = await asyncio.gather(
                accurate_future,
                timestamp_future
            )

        # ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        accurate_text = accurate_response.text if hasattr(
            accurate_response, 'text'
        ) else ""

        # í…ìŠ¤íŠ¸ ì •ì œ
        cleaned_text = _clean_transcript_text(accurate_text)

        # ë‘ ê²°ê³¼ ë³‘í•©
        merged = _merge_transcripts(cleaned_text, timestamp_response)

        # ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ë„ ì •ì œ
        cleaned_segments = []
        for seg in merged["segments"]:
            cleaned_segments.append({
                "start": seg["start"],
                "end": seg["end"],
                "text": _clean_transcript_text(seg["text"])
            })

        print(f"[STT] í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ ì™„ë£Œ: {len(cleaned_text)}ì, {len(cleaned_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        logger.info(
            f"[STT] í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ ì™„ë£Œ: {len(cleaned_text)}ì, "
            f"{len(cleaned_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸"
        )

        return {
            "full_text": cleaned_text,
            "segments": cleaned_segments,
            "language": language,
            "duration": merged["duration"]
        }

    except AudioFileError:
        raise

    except Exception as e:
        logger.error(f"[STT] ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜: {e}")
        raise TranscriptionError(f"ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
